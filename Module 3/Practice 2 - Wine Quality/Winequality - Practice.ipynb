{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Wine Quality.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _\"Quality ratings of Portuguese white wines\" (Classification task)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "\n",
    "## Part 0: Introduction\n",
    "\n",
    "### Overview\n",
    "The dataset that's we see here contains 12 columns and 4898 entries of data about Portuguese white wines.\n",
    "    \n",
    "**Метаданные:**\n",
    "    \n",
    "* **fixed acidity** \n",
    "\n",
    "* **volatile acidity**\n",
    "\n",
    "* **citric acid** \n",
    "\n",
    "* **residual sugar** \n",
    "\n",
    "* **chlorides** \n",
    "\n",
    "* **free sulfur dioxide** \n",
    "\n",
    "* **total sulfur dioxide**\n",
    "\n",
    "* **density** \n",
    "\n",
    "* **pH** \n",
    "\n",
    "* **sulphates** \n",
    "\n",
    "* **alcohol** \n",
    "\n",
    "* **quality** - score between 3 and 9\n",
    "\n",
    "\n",
    "### Questions:\n",
    "    \n",
    "Predict which wines are 'Good/1' and 'Not Good/0' (use binary classification; check balance of classes; calculate perdictions; choose the best model)\n",
    "\n",
    "\n",
    "## [Part 1: Import, Load Data](#Part-1:-Import,-Load-Data.)\n",
    "* ### Import libraries, Read data from ‘.csv’ file\n",
    "\n",
    "## [Part 2: Exploratory Data Analysis](#Part-2:-Exploratory-Data-Analysis.)\n",
    "* ### Info, Head, Describe\n",
    "* ### Encoding 'quality' attribute\n",
    "* ### 'quality' attribute value counts and visualisation\n",
    "* ### Resampling of an imbalanced dataset\n",
    "* ### Random under-sampling of an imbalanced dataset\n",
    "* ### Random over-sampling of an imbalanced dataset\n",
    "* ### Initialisation of target\n",
    "* ### Drop column 'quality'\n",
    "\n",
    "## [Part 3: Data Wrangling and Transformation](#Part-3:-Data-Wrangling-and-Transformation.)\n",
    "* ### StandardScaler\n",
    "* ### Creating datasets for ML part\n",
    "* ### 'Train\\Test' splitting method\n",
    "\n",
    "## [Part 4: Machine Learning](#Part-4:-Machine-Learning.)\n",
    "* ### Build, train and evaluate models without hyperparameters\n",
    "    * #### Logistic Regression, K-Nearest Neighbors, Decision Trees \n",
    "    * #### Classification report\n",
    "    * #### Confusion Matrix\n",
    "    * #### ROC-AUC score\n",
    "* ### Build, train and evaluate models with hyperparameters\n",
    "    * #### Logistic Regression, K-Nearest Neighbors, Decision Trees \n",
    "    * #### Classification report\n",
    "    * #### Confusion Matrix\n",
    "    * #### ROC-AUC score\n",
    "\n",
    "## [Conclusion](#Conclusion.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Import, Load Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import accuracy_score as accuracy, confusion_matrix as conf_matr\n",
    "from sklearn.metrics import classification_report as class_rep, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Read data from ‘.csv’ file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from '.csv' file\n",
    "df = pd.read_csv('winequality.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exploratory Data Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4898 entries, 0 to 4897\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         4898 non-null   float64\n",
      " 1   volatile acidity      4898 non-null   float64\n",
      " 2   citric acid           4898 non-null   float64\n",
      " 3   residual sugar        4898 non-null   float64\n",
      " 4   chlorides             4898 non-null   float64\n",
      " 5   free sulfur dioxide   4898 non-null   float64\n",
      " 6   total sulfur dioxide  4898 non-null   float64\n",
      " 7   density               4898 non-null   float64\n",
      " 8   pH                    4898 non-null   float64\n",
      " 9   sulphates             4898 non-null   float64\n",
      " 10  alcohol               4898 non-null   float64\n",
      " 11  quality               4898 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 459.3 KB\n"
     ]
    }
   ],
   "source": [
    "# print the full summary of the dataset  \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview of the first 5 lines of the loaded data \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.854788</td>\n",
       "      <td>0.278241</td>\n",
       "      <td>0.334192</td>\n",
       "      <td>6.391415</td>\n",
       "      <td>0.045772</td>\n",
       "      <td>35.308085</td>\n",
       "      <td>138.360657</td>\n",
       "      <td>0.994027</td>\n",
       "      <td>3.188267</td>\n",
       "      <td>0.489847</td>\n",
       "      <td>10.514267</td>\n",
       "      <td>5.877909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.843868</td>\n",
       "      <td>0.100795</td>\n",
       "      <td>0.121020</td>\n",
       "      <td>5.072058</td>\n",
       "      <td>0.021848</td>\n",
       "      <td>17.007137</td>\n",
       "      <td>42.498065</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>0.151001</td>\n",
       "      <td>0.114126</td>\n",
       "      <td>1.230621</td>\n",
       "      <td>0.885639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.800000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.987110</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.300000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>0.991723</td>\n",
       "      <td>3.090000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>0.993740</td>\n",
       "      <td>3.180000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.300000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>0.996100</td>\n",
       "      <td>3.280000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>11.400000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.200000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.660000</td>\n",
       "      <td>65.800000</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>1.038980</td>\n",
       "      <td>3.820000</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    4898.000000       4898.000000  4898.000000     4898.000000   \n",
       "mean        6.854788          0.278241     0.334192        6.391415   \n",
       "std         0.843868          0.100795     0.121020        5.072058   \n",
       "min         3.800000          0.080000     0.000000        0.600000   \n",
       "25%         6.300000          0.210000     0.270000        1.700000   \n",
       "50%         6.800000          0.260000     0.320000        5.200000   \n",
       "75%         7.300000          0.320000     0.390000        9.900000   \n",
       "max        14.200000          1.100000     1.660000       65.800000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  4898.000000          4898.000000           4898.000000  4898.000000   \n",
       "mean      0.045772            35.308085            138.360657     0.994027   \n",
       "std       0.021848            17.007137             42.498065     0.002991   \n",
       "min       0.009000             2.000000              9.000000     0.987110   \n",
       "25%       0.036000            23.000000            108.000000     0.991723   \n",
       "50%       0.043000            34.000000            134.000000     0.993740   \n",
       "75%       0.050000            46.000000            167.000000     0.996100   \n",
       "max       0.346000           289.000000            440.000000     1.038980   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  4898.000000  4898.000000  4898.000000  4898.000000  \n",
       "mean      3.188267     0.489847    10.514267     5.877909  \n",
       "std       0.151001     0.114126     1.230621     0.885639  \n",
       "min       2.720000     0.220000     8.000000     3.000000  \n",
       "25%       3.090000     0.410000     9.500000     5.000000  \n",
       "50%       3.180000     0.470000    10.400000     6.000000  \n",
       "75%       3.280000     0.550000    11.400000     6.000000  \n",
       "max       3.820000     1.080000    14.200000     9.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Encoding 'quality' attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda function; wine quality from 3-6 == 0, from 7-9 == 1.\n",
    "df['quality'] = df['quality'].apply(lambda x: 0 if x <=6 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        0  \n",
       "1      9.5        0  \n",
       "2     10.1        0  \n",
       "3      9.9        0  \n",
       "4      9.9        0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview of the first 5 lines of the loaded data \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### 'quality' attribute value counts and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3838\n",
       "1    1060\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualisation plot\n",
    "df['quality'].value_counts().plot(x=df['quality'], kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Resampling of an imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class count\n",
    "class_0_count, class_1_count =df['quality'].value_counts()\n",
    "\n",
    "# divide by class\n",
    "class_0 = df[df.quality == 0]\n",
    "class_1 = df[df.quality == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Random under-sampling of an imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD3CAYAAAD10FRmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAQeklEQVR4nO3dX2yTdd/H8U+3lon3MLrZa+JCSLwlWSbIEgk6/5SggRbmonQcwIiLiYY/Kirq4sI2F6IEMHvYiRlH6MFCTMaEAUvTacQs4jCBHUiQmRizLbpp201UNrZl7focPM/Tx2XcQLu6rvu9X0f016tc38Iv7zUXbbFFo9GoAABGyUj1AACA2Uf8AcBAxB8ADET8AcBAxB8ADET8AcBAxB8ADGRP9QC36+rVEU1O8pGEZMjNzdbQ0HCqxwCmYW8mT0aGTffc86//eH/axH9yMkr8k4g/S8xV7M3ZwWUfADAQ8QcAAxF/ADAQ8QcAAxF/ADAQ8QcAAxF/ADBQ2rzPPx0sumuh7shKjz9Sp3NRqke4pbHxsK79NZrqMeYF9mZyzYe9mR67IU3ckWVX6dunUj3GvHHmv57TtVQPMU+wN5NrPuxNLvsAgIGIPwAYiPgDgIGIPwAYiPgDgIGIPwAY6LbiPzw8rGeffVa//PKLJKmzs1OlpaVav369GhoaYsd1d3errKxMbrdb1dXVCofDkqSBgQFt27ZNHo9Hu3bt0sjIyD/wVAAAt+uW8f/uu++0detW9fb2SpLGxsa0d+9eNTY2yufz6fLly+ro6JAkVVZWqra2Vu3t7YpGo2pubpYk7du3T+Xl5fL7/Vq+fLkaGxv/uWcEALilW8a/ublZdXV1sixLknTp0iUtXbpUS5Yskd1uV2lpqfx+v/r7+zU2NqaioiJJktfrld/v18TEhC5cuCC32z1lHQCQOrf8hO/+/fun3A4Gg3I6nbHblmUpEAhMW3c6nQoEArp69aqys7Nlt9unrAMAUifur3eIRqf//5o2my3u9Xjl5mbH/Rikv3T4nheYKd33Ztzxz8vL0+DgYOx2MBiUZVnT1kOhkCzLUk5OjoaHhxWJRJSZmRlbj9fQ0PCc/4+d030zzEWhULp/g8rcwN5Mvrm+NzMybDd90Rz3Wz1Xrlypnp4e9fX1KRKJqK2tTS6XS/n5+crKylJXV5ckqbW1VS6XSw6HQ6tWrZLP55uyDgBInbhf+WdlZengwYPavXu3xsfHtWbNGnk8HklSfX29ampqNDIyosLCQlVUVEiS6urqVFVVpSNHjmjx4sU6fPhwcp8FACAutx3/s2fPxn5dXFys06dPTzumoKBALS0t09bz8/PV1NSU4IgAgGTjE74AYCDiDwAGIv4AYCDiDwAGIv4AYCDiDwAGIv4AYCDiDwAGIv4AYCDiDwAGIv4AYCDiDwAGIv4AYCDiDwAGIv4AYCDiDwAGIv4AYCDiDwAGIv4AYCDiDwAGIv4AYCDiDwAGIv4AYCDiDwAGIv4AYCDiDwAGIv4AYCDiDwAGIv4AYCDiDwAGmlH8T506pZKSEpWUlOjQoUOSpO7ubpWVlcntdqu6ulrhcFiSNDAwoG3btsnj8WjXrl0aGRmZ+fQAgIQkHP/R0VHt379fTU1NOnXqlC5evKjOzk5VVlaqtrZW7e3tikajam5uliTt27dP5eXl8vv9Wr58uRobG5P2JAAA8Uk4/pFIRJOTkxodHVU4HFY4HJbdbtfY2JiKiookSV6vV36/XxMTE7pw4YLcbveUdQBAatgTfWB2drbeeOMNbdiwQXfccYdWr14th8Mhp9MZO8bpdCoQCOjq1avKzs6W3W6fsg4ASI2E4//DDz/os88+01dffaVFixbpnXfe0TfffDPtOJvNpmg0esP1eOTmZic6KtKY07ko1SMAN5TuezPh+J87d07FxcXKzc2V9D+Xco4eParBwcHYMaFQSJZlKScnR8PDw4pEIsrMzIytx2NoaFiTk9N/iMwl6b4Z5qJQ6FqqR5gX2JvJN9f3ZkaG7aYvmhO+5l9QUKDOzk5dv35d0WhUZ8+e1erVq5WVlaWuri5JUmtrq1wulxwOh1atWiWfzzdlHQCQGgm/8n/yySd15coVeb1eORwOrVixQtu3b9e6detUU1OjkZERFRYWqqKiQpJUV1enqqoqHTlyRIsXL9bhw4eT9iQAAPFJOP6StH37dm3fvn3KWkFBgVpaWqYdm5+fr6amppmcDgCQJHzCFwAMRPwBwEDEHwAMRPwBwEDEHwAMRPwBwEDEHwAMRPwBwEDEHwAMRPwBwEDEHwAMRPwBwEDEHwAMRPwBwEDEHwAMRPwBwEDEHwAMRPwBwEDEHwAMRPwBwEDEHwAMRPwBwEDEHwAMRPwBwEDEHwAMRPwBwEDEHwAMRPwBwEDEHwAMRPwBwEAziv/Zs2fl9Xrl8Xj0wQcfSJI6OztVWlqq9evXq6GhIXZsd3e3ysrK5Ha7VV1drXA4PLPJAQAJSzj+P//8s+rq6tTY2KgzZ87oypUr6ujo0N69e9XY2Cifz6fLly+ro6NDklRZWana2lq1t7crGo2qubk5aU8CABCfhOP/xRdfaOPGjbrvvvvkcDjU0NCghQsXaunSpVqyZInsdrtKS0vl9/vV39+vsbExFRUVSZK8Xq/8fn+yngMAIE72RB/Y19cnh8Ohl156SaFQSGvXrtWyZcvkdDpjx1iWpUAgoGAwOGXd6XQqEAjMbHIAQMISjn8kEtHFixfV1NSkO++8U6+88ooWLlw47TibzaZoNHrD9Xjk5mYnOirSmNO5KNUjADeU7nsz4fjfe++9Ki4uVk5OjiTpmWeekd/vV2ZmZuyYYDAoy7KUl5enwcHB2HooFJJlWXGdb2hoWJOT03+IzCXpvhnmolDoWqpHmBfYm8k31/dmRobtpi+aE77mv3btWp07d05//fWXIpGIvv76a3k8HvX09Kivr0+RSERtbW1yuVzKz89XVlaWurq6JEmtra1yuVyJnhoAMEMJv/JfuXKlXn75ZZWXl2tiYkJPPPGEtm7dqgceeEC7d+/W+Pi41qxZI4/HI0mqr69XTU2NRkZGVFhYqIqKiqQ9CQBAfBKOvyRt3rxZmzdvnrJWXFys06dPTzu2oKBALS0tMzkdACBJ+IQvABiI+AOAgYg/ABiI+AOAgYg/ABiI+AOAgYg/ABiI+AOAgYg/ABiI+AOAgYg/ABiI+AOAgYg/ABiI+AOAgYg/ABiI+AOAgYg/ABiI+AOAgYg/ABiI+AOAgYg/ABiI+AOAgYg/ABiI+AOAgYg/ABiI+AOAgYg/ABiI+AOAgYg/ABiI+AOAgWYc/0OHDqmqqkqS1N3drbKyMrndblVXVyscDkuSBgYGtG3bNnk8Hu3atUsjIyMzPS0AYAZmFP/z58/r5MmTsduVlZWqra1Ve3u7otGompubJUn79u1TeXm5/H6/li9frsbGxplNDQCYkYTj/8cff6ihoUE7d+6UJPX392tsbExFRUWSJK/XK7/fr4mJCV24cEFut3vKOgAgdRKO/3vvvac9e/borrvukiQFg0E5nc7Y/U6nU4FAQFevXlV2drbsdvuUdQBA6tgTedDx48e1ePFiFRcX68SJE5KkaDQ67TibzfYf1+OVm5sd/6BIe07nolSPANxQuu/NhOLv8/kUCoX03HPP6c8//9T169dls9k0ODgYOyYUCsmyLOXk5Gh4eFiRSESZmZmx9XgNDQ1rcnL6D5K5JN03w1wUCl1L9QjzAnsz+eb63szIsN30RXNCl30++eQTtbW16dSpU3r99df19NNP68CBA8rKylJXV5ckqbW1VS6XSw6HQ6tWrZLP55uyDgBInaS+z7++vl4HDhzQhg0bNDo6qoqKCklSXV2dmpubtXHjRl28eFFvvvlmMk8LAIhTQpd9/s7r9crr9UqSCgoK1NLSMu2Y/Px8NTU1zfRUAIAk4RO+AGAg4g8ABiL+AGAg4g8ABiL+AGAg4g8ABiL+AGAg4g8ABiL+AGAg4g8ABiL+AGAg4g8ABiL+AGAg4g8ABiL+AGAg4g8ABiL+AGAg4g8ABiL+AGAg4g8ABiL+AGAg4g8ABiL+AGAg4g8ABiL+AGAg4g8ABiL+AGAg4g8ABiL+AGAg4g8ABiL+AGCgGcX/o48+UklJiUpKSvThhx9Kkjo7O1VaWqr169eroaEhdmx3d7fKysrkdrtVXV2tcDg8s8kBAAlLOP6dnZ06d+6cTp48qdbWVn3//fdqa2vT3r171djYKJ/Pp8uXL6ujo0OSVFlZqdraWrW3tysajaq5uTlpTwIAEJ+E4+90OlVVVaUFCxbI4XDo3//+t3p7e7V06VItWbJEdrtdpaWl8vv96u/v19jYmIqKiiRJXq9Xfr8/Wc8BABAne6IPXLZsWezXvb298vl8euGFF+R0OmPrlmUpEAgoGAxOWXc6nQoEAnGdLzc3O9FRkcaczkWpHgG4oXTfmwnH///8+OOP2rFjh959913Z7Xb19PRMud9msykajU57nM1mi+s8Q0PDmpyc/vvMJem+GeaiUOhaqkeYF9ibyTfX92ZGhu2mL5pn9A++XV1devHFF/X2229r06ZNysvL0+DgYOz+YDAoy7KmrYdCIVmWNZNTAwBmIOH4//rrr3r11VdVX1+vkpISSdLKlSvV09Ojvr4+RSIRtbW1yeVyKT8/X1lZWerq6pIktba2yuVyJecZAADilvBln6NHj2p8fFwHDx6MrW3ZskUHDx7U7t27NT4+rjVr1sjj8UiS6uvrVVNTo5GRERUWFqqiomLm0wMAEpJw/GtqalRTU3PD+06fPj1traCgQC0tLYmeDgCQRHzCFwAMRPwBwEDEHwAMRPwBwEDEHwAMRPwBwEDEHwAMRPwBwEDEHwAMRPwBwEDEHwAMRPwBwEDEHwAMRPwBwEDEHwAMRPwBwEDEHwAMRPwBwEDEHwAMRPwBwEDEHwAMRPwBwEDEHwAMRPwBwEDEHwAMRPwBwEDEHwAMRPwBwEDEHwAMRPwBwECzGv8zZ85o48aNWrdunY4dOzabpwYA/I19tk4UCATU0NCgEydOaMGCBdqyZYseffRRPfjgg7M1AgDgf81a/Ds7O/XYY4/p7rvvliS53W75/X699tprt/X4jAzbPzhd8lj3LEz1CPNKuvy9pwP2ZnLN9b15q/lmLf7BYFBOpzN227IsXbp06bYff889//onxkq6ozXrUz3CvJKbm53qEeYN9mZypfvenLVr/tFodNqazTa3f3ICwHw1a/HPy8vT4OBg7HYwGJRlWbN1egDA38xa/B9//HGdP39ev//+u0ZHR/X555/L5XLN1ukBAH8za9f88/LytGfPHlVUVGhiYkKbN2/Www8/PFunBwD8jS16o4vxAIB5jU/4AoCBiD8AGIj4A4CBiD8AGIj4A4CBZu2tnkiNn376Se3t7frtt9+UkZEhy7L01FNPacWKFakeDUAK8cp/Hjt27JjeeustSdKKFSv00EMPSZJqa2v18ccfp3I0ACnG+/znMbfbrdbWVi1cOPXbHEdHR7Vp0yb5/f4UTQZIAwMDN73//vvvn6VJzMRln3nMbrcrHA5PWx8bG5PD4UjBRMD/27Fjh3p7e2VZ1rQvfrTZbPryyy9TNJkZiP88tnPnTj3//PMqLi6OfZ12KBTSt99+qz179qR4Opju008/VXl5uerq6vTII4+kehzjcNlnngsEAjp//ryCwaCi0ajy8vJUXFysvLy8VI8G6NKlSzp+/Ljef//9VI9iHOIPAAbi3T4AYCDiDwAGIv4AYCDiDwAGIv4AYKD/BmTydpZWMd7VAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_0_under = class_0.sample(class_1_count)\n",
    "df_under = pd.concat([class_0_under, class_1], axis=0)\n",
    "df_under['quality'].value_counts().plot(x=df_under['quality'], kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Random over-sampling of an imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_1_over = class_1.sample(class_0_count, replace=True)\n",
    "df_over = pd.concat([class_0, class_1_over], axis=0)\n",
    "df_over['quality'].value_counts().plot(x=df_over['quality'], kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Initialisation of target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['quality']\n",
    "target_under = df_under['quality']\n",
    "target_over = df_over['quality']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Drop column 'quality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.drop(['quality'], axis=1).copy()\n",
    "dataset_under = df_under.drop(['quality'], axis=1).copy()\n",
    "dataset_over = df_over.drop(['quality'], axis=1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Wrangling and Transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.172097</td>\n",
       "      <td>-0.081770</td>\n",
       "      <td>0.213280</td>\n",
       "      <td>2.821349</td>\n",
       "      <td>-0.035355</td>\n",
       "      <td>0.569932</td>\n",
       "      <td>0.744565</td>\n",
       "      <td>2.331512</td>\n",
       "      <td>-1.246921</td>\n",
       "      <td>-0.349184</td>\n",
       "      <td>-1.393152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.657501</td>\n",
       "      <td>0.215896</td>\n",
       "      <td>0.048001</td>\n",
       "      <td>-0.944765</td>\n",
       "      <td>0.147747</td>\n",
       "      <td>-1.253019</td>\n",
       "      <td>-0.149685</td>\n",
       "      <td>-0.009154</td>\n",
       "      <td>0.740029</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>-0.824276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.475751</td>\n",
       "      <td>0.017452</td>\n",
       "      <td>0.543838</td>\n",
       "      <td>0.100282</td>\n",
       "      <td>0.193523</td>\n",
       "      <td>-0.312141</td>\n",
       "      <td>-0.973336</td>\n",
       "      <td>0.358665</td>\n",
       "      <td>0.475102</td>\n",
       "      <td>-0.436816</td>\n",
       "      <td>-0.336667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.409125</td>\n",
       "      <td>-0.478657</td>\n",
       "      <td>-0.117278</td>\n",
       "      <td>0.415768</td>\n",
       "      <td>0.559727</td>\n",
       "      <td>0.687541</td>\n",
       "      <td>1.121091</td>\n",
       "      <td>0.525855</td>\n",
       "      <td>0.011480</td>\n",
       "      <td>-0.787342</td>\n",
       "      <td>-0.499203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.409125</td>\n",
       "      <td>-0.478657</td>\n",
       "      <td>-0.117278</td>\n",
       "      <td>0.415768</td>\n",
       "      <td>0.559727</td>\n",
       "      <td>0.687541</td>\n",
       "      <td>1.121091</td>\n",
       "      <td>0.525855</td>\n",
       "      <td>0.011480</td>\n",
       "      <td>-0.787342</td>\n",
       "      <td>-0.499203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.172097 -0.081770  0.213280  2.821349 -0.035355  0.569932  0.744565   \n",
       "1 -0.657501  0.215896  0.048001 -0.944765  0.147747 -1.253019 -0.149685   \n",
       "2  1.475751  0.017452  0.543838  0.100282  0.193523 -0.312141 -0.973336   \n",
       "3  0.409125 -0.478657 -0.117278  0.415768  0.559727  0.687541  1.121091   \n",
       "4  0.409125 -0.478657 -0.117278  0.415768  0.559727  0.687541  1.121091   \n",
       "\n",
       "         7         8         9         10  \n",
       "0  2.331512 -1.246921 -0.349184 -1.393152  \n",
       "1 -0.009154  0.740029  0.001342 -0.824276  \n",
       "2  0.358665  0.475102 -0.436816 -0.336667  \n",
       "3  0.525855  0.011480 -0.787342 -0.499203  \n",
       "4  0.525855  0.011480 -0.787342 -0.499203  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# StandardScaler \n",
    "st_sc = StandardScaler()\n",
    "\n",
    "dataset_st = st_sc.fit_transform(dataset)\n",
    "dataset_st = pd.DataFrame(dataset_st)\n",
    "dataset_st.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.120842</td>\n",
       "      <td>-0.548675</td>\n",
       "      <td>1.500499</td>\n",
       "      <td>0.175202</td>\n",
       "      <td>0.943190</td>\n",
       "      <td>0.910517</td>\n",
       "      <td>0.828907</td>\n",
       "      <td>0.684349</td>\n",
       "      <td>2.253008</td>\n",
       "      <td>0.053298</td>\n",
       "      <td>-0.399630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.490605</td>\n",
       "      <td>-1.268375</td>\n",
       "      <td>0.464753</td>\n",
       "      <td>-0.971729</td>\n",
       "      <td>-0.262964</td>\n",
       "      <td>-1.043934</td>\n",
       "      <td>-0.607409</td>\n",
       "      <td>-0.573058</td>\n",
       "      <td>-0.071780</td>\n",
       "      <td>-0.526570</td>\n",
       "      <td>0.206670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.248922</td>\n",
       "      <td>-0.651489</td>\n",
       "      <td>-0.382675</td>\n",
       "      <td>-0.853081</td>\n",
       "      <td>-0.841919</td>\n",
       "      <td>-0.005632</td>\n",
       "      <td>-0.161656</td>\n",
       "      <td>-0.532192</td>\n",
       "      <td>1.123825</td>\n",
       "      <td>-0.112379</td>\n",
       "      <td>0.282457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.988448</td>\n",
       "      <td>-0.343046</td>\n",
       "      <td>0.088118</td>\n",
       "      <td>-1.011279</td>\n",
       "      <td>-0.359457</td>\n",
       "      <td>1.032670</td>\n",
       "      <td>1.225132</td>\n",
       "      <td>-0.802535</td>\n",
       "      <td>1.588783</td>\n",
       "      <td>-0.443731</td>\n",
       "      <td>0.471926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.490605</td>\n",
       "      <td>1.918870</td>\n",
       "      <td>3.007039</td>\n",
       "      <td>0.847541</td>\n",
       "      <td>0.315990</td>\n",
       "      <td>2.498508</td>\n",
       "      <td>1.423244</td>\n",
       "      <td>1.281617</td>\n",
       "      <td>-0.271048</td>\n",
       "      <td>-0.609408</td>\n",
       "      <td>-1.612231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.120842 -0.548675  1.500499  0.175202  0.943190  0.910517  0.828907   \n",
       "1  0.490605 -1.268375  0.464753 -0.971729 -0.262964 -1.043934 -0.607409   \n",
       "2 -0.248922 -0.651489 -0.382675 -0.853081 -0.841919 -0.005632 -0.161656   \n",
       "3 -0.988448 -0.343046  0.088118 -1.011279 -0.359457  1.032670  1.225132   \n",
       "4  0.490605  1.918870  3.007039  0.847541  0.315990  2.498508  1.423244   \n",
       "\n",
       "         7         8         9         10  \n",
       "0  0.684349  2.253008  0.053298 -0.399630  \n",
       "1 -0.573058 -0.071780 -0.526570  0.206670  \n",
       "2 -0.532192  1.123825 -0.112379  0.282457  \n",
       "3 -0.802535  1.588783 -0.443731  0.471926  \n",
       "4  1.281617 -0.271048 -0.609408 -1.612231  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_sc = StandardScaler()\n",
    "dataset_under_st = st_sc.fit_transform(dataset_under)\n",
    "dataset_under_st = pd.DataFrame(dataset_under_st)\n",
    "dataset_under_st.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.247899</td>\n",
       "      <td>-0.036925</td>\n",
       "      <td>0.270581</td>\n",
       "      <td>3.017307</td>\n",
       "      <td>0.098470</td>\n",
       "      <td>0.611606</td>\n",
       "      <td>0.918285</td>\n",
       "      <td>2.484988</td>\n",
       "      <td>-1.284286</td>\n",
       "      <td>-0.378876</td>\n",
       "      <td>-1.538691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.619437</td>\n",
       "      <td>0.266975</td>\n",
       "      <td>0.085862</td>\n",
       "      <td>-0.913282</td>\n",
       "      <td>0.308212</td>\n",
       "      <td>-1.328347</td>\n",
       "      <td>-0.039466</td>\n",
       "      <td>0.176577</td>\n",
       "      <td>0.656963</td>\n",
       "      <td>-0.049758</td>\n",
       "      <td>-1.010022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.610855</td>\n",
       "      <td>0.064375</td>\n",
       "      <td>0.640018</td>\n",
       "      <td>0.177405</td>\n",
       "      <td>0.360647</td>\n",
       "      <td>-0.327081</td>\n",
       "      <td>-0.921604</td>\n",
       "      <td>0.539327</td>\n",
       "      <td>0.398130</td>\n",
       "      <td>-0.461155</td>\n",
       "      <td>-0.556878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.495709</td>\n",
       "      <td>-0.442125</td>\n",
       "      <td>-0.098857</td>\n",
       "      <td>0.506669</td>\n",
       "      <td>0.780131</td>\n",
       "      <td>0.736764</td>\n",
       "      <td>1.321548</td>\n",
       "      <td>0.704214</td>\n",
       "      <td>-0.054828</td>\n",
       "      <td>-0.790273</td>\n",
       "      <td>-0.707926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.495709</td>\n",
       "      <td>-0.442125</td>\n",
       "      <td>-0.098857</td>\n",
       "      <td>0.506669</td>\n",
       "      <td>0.780131</td>\n",
       "      <td>0.736764</td>\n",
       "      <td>1.321548</td>\n",
       "      <td>0.704214</td>\n",
       "      <td>-0.054828</td>\n",
       "      <td>-0.790273</td>\n",
       "      <td>-0.707926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.247899 -0.036925  0.270581  3.017307  0.098470  0.611606  0.918285   \n",
       "1 -0.619437  0.266975  0.085862 -0.913282  0.308212 -1.328347 -0.039466   \n",
       "2  1.610855  0.064375  0.640018  0.177405  0.360647 -0.327081 -0.921604   \n",
       "3  0.495709 -0.442125 -0.098857  0.506669  0.780131  0.736764  1.321548   \n",
       "4  0.495709 -0.442125 -0.098857  0.506669  0.780131  0.736764  1.321548   \n",
       "\n",
       "         7         8         9         10  \n",
       "0  2.484988 -1.284286 -0.378876 -1.538691  \n",
       "1  0.176577  0.656963 -0.049758 -1.010022  \n",
       "2  0.539327  0.398130 -0.461155 -0.556878  \n",
       "3  0.704214 -0.054828 -0.790273 -0.707926  \n",
       "4  0.704214 -0.054828 -0.790273 -0.707926  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_sc = StandardScaler()\n",
    "dataset_over_st = st_sc.fit_transform(dataset_over)\n",
    "dataset_over_st = pd.DataFrame(dataset_over_st)\n",
    "dataset_over_st.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Creating datasets for ML part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set 'X' for features' and y' for the target ('quality').\n",
    "X = dataset_st.copy()\n",
    "y = target\n",
    "\n",
    "# for under-sampling dataset \n",
    "X_under = dataset_under_st.copy()\n",
    "y_under = target_under\n",
    "\n",
    "# for over-sampling dataset \n",
    "X_over = dataset_over_st.copy()\n",
    "y_over = target_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.172097</td>\n",
       "      <td>-0.081770</td>\n",
       "      <td>0.213280</td>\n",
       "      <td>2.821349</td>\n",
       "      <td>-0.035355</td>\n",
       "      <td>0.569932</td>\n",
       "      <td>0.744565</td>\n",
       "      <td>2.331512</td>\n",
       "      <td>-1.246921</td>\n",
       "      <td>-0.349184</td>\n",
       "      <td>-1.393152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.657501</td>\n",
       "      <td>0.215896</td>\n",
       "      <td>0.048001</td>\n",
       "      <td>-0.944765</td>\n",
       "      <td>0.147747</td>\n",
       "      <td>-1.253019</td>\n",
       "      <td>-0.149685</td>\n",
       "      <td>-0.009154</td>\n",
       "      <td>0.740029</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>-0.824276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.475751</td>\n",
       "      <td>0.017452</td>\n",
       "      <td>0.543838</td>\n",
       "      <td>0.100282</td>\n",
       "      <td>0.193523</td>\n",
       "      <td>-0.312141</td>\n",
       "      <td>-0.973336</td>\n",
       "      <td>0.358665</td>\n",
       "      <td>0.475102</td>\n",
       "      <td>-0.436816</td>\n",
       "      <td>-0.336667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.409125</td>\n",
       "      <td>-0.478657</td>\n",
       "      <td>-0.117278</td>\n",
       "      <td>0.415768</td>\n",
       "      <td>0.559727</td>\n",
       "      <td>0.687541</td>\n",
       "      <td>1.121091</td>\n",
       "      <td>0.525855</td>\n",
       "      <td>0.011480</td>\n",
       "      <td>-0.787342</td>\n",
       "      <td>-0.499203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.409125</td>\n",
       "      <td>-0.478657</td>\n",
       "      <td>-0.117278</td>\n",
       "      <td>0.415768</td>\n",
       "      <td>0.559727</td>\n",
       "      <td>0.687541</td>\n",
       "      <td>1.121091</td>\n",
       "      <td>0.525855</td>\n",
       "      <td>0.011480</td>\n",
       "      <td>-0.787342</td>\n",
       "      <td>-0.499203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.172097 -0.081770  0.213280  2.821349 -0.035355  0.569932  0.744565   \n",
       "1 -0.657501  0.215896  0.048001 -0.944765  0.147747 -1.253019 -0.149685   \n",
       "2  1.475751  0.017452  0.543838  0.100282  0.193523 -0.312141 -0.973336   \n",
       "3  0.409125 -0.478657 -0.117278  0.415768  0.559727  0.687541  1.121091   \n",
       "4  0.409125 -0.478657 -0.117278  0.415768  0.559727  0.687541  1.121091   \n",
       "\n",
       "         7         8         9         10  \n",
       "0  2.331512 -1.246921 -0.349184 -1.393152  \n",
       "1 -0.009154  0.740029  0.001342 -0.824276  \n",
       "2  0.358665  0.475102 -0.436816 -0.336667  \n",
       "3  0.525855  0.011480 -0.787342 -0.499203  \n",
       "4  0.525855  0.011480 -0.787342 -0.499203  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview of the first 5 lines of the loaded data \n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### 'Train\\Test' split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply 'Train\\Test' splitting method\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_under, X_test_under, y_train_under, y_test_under = train_test_split(X_under, y_under, test_size=0.2, \n",
    "                                                                            random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_over, X_test_over, y_train_over, y_test_over = train_test_split(X_over, y_over, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3918, 11), (3918,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print shape of X_train and y_train\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((980, 11), (980,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print shape of X_test and y_test\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Build, train and evaluate models without hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Logistic Regression\n",
    "* K-Nearest Neighbors\n",
    "* Decision Trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "LR = LogisticRegression()\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "KNN = KNeighborsClassifier()\n",
    "\n",
    "# Decision Tree\n",
    "DT = DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR.fit(X_train, y_train)\n",
    "LR_preds = LR.predict(X_test)\n",
    "\n",
    "LR.fit(X_train_under, y_train_under)\n",
    "LR_preds_under = LR.predict(X_test_under)\n",
    "\n",
    "LR.fit(X_train_over, y_train_over)\n",
    "LR_preds_over = LR.predict(X_test_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN.fit(X_train, y_train)\n",
    "KNN_preds = KNN.predict(X_test)\n",
    "\n",
    "KNN.fit(X_train_under, y_train_under)\n",
    "KNN_preds_under = KNN.predict(X_test_under)\n",
    "\n",
    "KNN.fit(X_train_over, y_train_over)\n",
    "KNN_preds_over = KNN.predict(X_test_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT.fit(X_train, y_train)\n",
    "DT_preds = DT.predict(X_test)\n",
    "\n",
    "DT.fit(X_train_under, y_train_under)\n",
    "DT_preds_under = DT.predict(X_test_under)\n",
    "\n",
    "DT.fit(X_train_over, y_train_over)\n",
    "DT_preds_over = DT.predict(X_test_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Sampling \n",
      "\n",
      "LR Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0   0.815611  0.943717  0.875000       764\n",
      "           1   0.552083  0.245370  0.339744       216\n",
      "\n",
      "    accuracy                       0.789796       980\n",
      "   macro avg   0.683847  0.594544  0.607372       980\n",
      "weighted avg   0.757527  0.789796  0.757025       980\n",
      "\n",
      "KNN Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0   0.873918  0.925393  0.898919       764\n",
      "           1   0.666667  0.527778  0.589147       216\n",
      "\n",
      "    accuracy                       0.837755       980\n",
      "   macro avg   0.770293  0.726585  0.744033       980\n",
      "weighted avg   0.828238  0.837755  0.830643       980\n",
      "\n",
      "DT Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0   0.901682  0.912304  0.906962       764\n",
      "           1   0.676329  0.648148  0.661939       216\n",
      "\n",
      "    accuracy                       0.854082       980\n",
      "   macro avg   0.789005  0.780226  0.784450       980\n",
      "weighted avg   0.852012  0.854082  0.852957       980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('No Sampling \\n')\n",
    "print(\"LR Classification Report: \\n\", class_rep(y_test, LR_preds, digits = 6))\n",
    "print(\"KNN Classification Report: \\n\", class_rep(y_test, KNN_preds, digits = 6))\n",
    "print(\"DT Classification Report: \\n\", class_rep(y_test, DT_preds, digits = 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under Sampling \n",
      "\n",
      "LR Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0   0.776699  0.720721  0.747664       222\n",
      "           1   0.715596  0.772277  0.742857       202\n",
      "\n",
      "    accuracy                       0.745283       424\n",
      "   macro avg   0.746148  0.746499  0.745260       424\n",
      "weighted avg   0.747589  0.745283  0.745374       424\n",
      "\n",
      "KNN Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0   0.765957  0.648649  0.702439       222\n",
      "           1   0.669492  0.782178  0.721461       202\n",
      "\n",
      "    accuracy                       0.712264       424\n",
      "   macro avg   0.717724  0.715413  0.711950       424\n",
      "weighted avg   0.720000  0.712264  0.711501       424\n",
      "\n",
      "DT Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0   0.826531  0.729730  0.775120       222\n",
      "           1   0.736842  0.831683  0.781395       202\n",
      "\n",
      "    accuracy                       0.778302       424\n",
      "   macro avg   0.781686  0.780706  0.778257       424\n",
      "weighted avg   0.783802  0.778302  0.778109       424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Under Sampling \\n')\n",
    "print(\"LR Classification Report: \\n\", class_rep(y_test_under, LR_preds_under, digits = 6))\n",
    "print(\"KNN Classification Report: \\n\", class_rep(y_test_under, KNN_preds_under, digits = 6))\n",
    "print(\"DT Classification Report: \\n\", class_rep(y_test_under, DT_preds_under, digits = 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over Sampling \n",
      "\n",
      "LR Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0   0.738162  0.691906  0.714286       766\n",
      "           1   0.711491  0.755844  0.732997       770\n",
      "\n",
      "    accuracy                       0.723958      1536\n",
      "   macro avg   0.724827  0.723875  0.723642      1536\n",
      "weighted avg   0.724792  0.723958  0.723666      1536\n",
      "\n",
      "KNN Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0   0.917981  0.759791  0.831429       766\n",
      "           1   0.796009  0.932468  0.858852       770\n",
      "\n",
      "    accuracy                       0.846354      1536\n",
      "   macro avg   0.856995  0.846129  0.845140      1536\n",
      "weighted avg   0.856836  0.846354  0.845176      1536\n",
      "\n",
      "DT Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0   0.975110  0.869452  0.919255       766\n",
      "           1   0.882767  0.977922  0.927911       770\n",
      "\n",
      "    accuracy                       0.923828      1536\n",
      "   macro avg   0.928938  0.923687  0.923583      1536\n",
      "weighted avg   0.928818  0.923828  0.923594      1536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Over Sampling \\n')\n",
    "print(\"LR Classification Report: \\n\", class_rep(y_test_over, LR_preds_over, digits = 6))\n",
    "print(\"KNN Classification Report: \\n\", class_rep(y_test_over, KNN_preds_over, digits = 6))\n",
    "print(\"DT Classification Report: \\n\", class_rep(y_test_over, DT_preds_over, digits = 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          \n",
      "              No Sampling  Under Sampling  Over Sampling  \n",
      "  LR Matrix   [[721  43]   [[160  62]      [[530 236]     \n",
      "               [163  53]]   [ 46 156]]      [188 582]]    \n",
      "                                                          \n",
      "  KNN Matrix  [[707  57]   [[144  78]      [[582 184]     \n",
      "               [102 114]]   [ 44 158]]      [ 52 718]]    \n",
      "                                                          \n",
      "  DT Matrix   [[697  67]   [[162  60]      [[666 100]     \n",
      "               [ 76 140]]   [ 34 168]]      [ 17 753]]    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from columnar import columnar\n",
    "\n",
    "data = [\n",
    "    ['', 'No Sampling', 'Under Sampling', 'Over Sampling'],\n",
    "    ['LR Matrix', conf_matr(y_test, LR_preds),conf_matr(y_test_under, LR_preds_under),conf_matr(y_test_over, LR_preds_over)],\n",
    "    ['','','',''],\n",
    "    ['KNN Matrix', conf_matr(y_test, KNN_preds), conf_matr(y_test_under, KNN_preds_under), conf_matr(y_test_over, KNN_preds_over)],\n",
    "    ['','','',''],\n",
    "    ['DT Matrix', conf_matr(y_test, DT_preds), conf_matr(y_test_under, DT_preds_under), conf_matr(y_test_over, DT_preds_over)]\n",
    "]\n",
    "\n",
    "table = columnar(data, no_borders=True)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### ROC-AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          \n",
      "               No Sampling         Under Sampling      Over Sampling       \n",
      "  LR roc-auc   0.5945438239286408  0.7464989742217465  0.7238750805330441  \n",
      "                                                                           \n",
      "  KNN roc-auc  0.7265852239674229  0.7154134332352154  0.8461293275914685  \n",
      "                                                                           \n",
      "  DT roc-auc   0.7802259065348071  0.7807064490232807  0.9236868875250077  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    ['', 'No Sampling', 'Under Sampling', 'Over Sampling'],\n",
    "    ['LR roc-auc', roc_auc_score(y_test, LR_preds),roc_auc_score(y_test_under, LR_preds_under),roc_auc_score(y_test_over, LR_preds_over)],\n",
    "    ['','','',''],\n",
    "    ['KNN roc-auc', roc_auc_score(y_test, KNN_preds), roc_auc_score(y_test_under, KNN_preds_under),roc_auc_score(y_test_over, KNN_preds_over)],\n",
    "    ['','','',''],\n",
    "    ['DT roc-auc', roc_auc_score(y_test, DT_preds), roc_auc_score(y_test_under, DT_preds_under),roc_auc_score(y_test_over, DT_preds_over)]\n",
    "]\n",
    "\n",
    "table = columnar(data, no_borders=True)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший набор данных - oversampling, лучшая модель - DecisionTree. Далее попробуем настроить гиперпараметры для всех трех моделей, но только на наборе oversampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Build, train and evaluate models with hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR гиперпараметры: \n",
      " {'C': 5, 'fit_intercept': False, 'penalty': 'l1', 'random_state': 0, 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "LR = LogisticRegression()\n",
    "LR_params = {'penalty':['l1', 'l2', 'elasticnet', 'none'],\n",
    "             'C':range(1, 12, 1), \n",
    "             'fit_intercept':[False],\n",
    "             'random_state':[0],\n",
    "             'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "LR_best = GridSearchCV(LR, param_grid = LR_params, n_jobs=-1)\n",
    "LR_best.fit(X_train_over, y_train_over)\n",
    "print('LR гиперпараметры: \\n', LR_best.best_params_)\n",
    "LR_preds = LR_best.predict(X_test_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN гиперпараметры: \n",
      " {'algorithm': 'auto', 'leaf_size': 30, 'n_neighbors': 5, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest Neighbors\n",
    "KNN = KNeighborsClassifier()\n",
    "KNN_params = {'n_neighbors':range(5, 15, 2),\n",
    "             'weights':['uniform', 'distance'],\n",
    "             'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "             'leaf_size':range(30, 60, 10)}\n",
    "KNN_best = GridSearchCV(KNN, param_grid = KNN_params, n_jobs=-1)             \n",
    "KNN_best.fit(X_train_over, y_train_over)\n",
    "print('KNN гиперпараметры: \\n', KNN_best.best_params_)\n",
    "KNN_preds_over = KNN_best.predict(X_test_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT гиперпараметры: \n",
      " {'criterion': 'gini', 'max_depth': 30, 'max_features': 'auto', 'random_state': 0, 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "DT = DecisionTreeClassifier()\n",
    "DT_params = {'criterion':['gini', 'entropy'],\n",
    "             'splitter':['best', 'random'], \n",
    "             'max_depth':range(5, 35, 5),\n",
    "             'max_features':['auto', 'sqrt','log2'], \n",
    "             'random_state':[0]}\n",
    "DT_best = GridSearchCV(DT, param_grid = DT_params, n_jobs=-1)\n",
    "DT_best.fit(X_train_over, y_train_over)\n",
    "print('DT гиперпараметры: \\n', DT_best.best_params_)\n",
    "DT_preds_over = DT_best.predict(X_test_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR params   {'C': 5, 'fit_intercept': False, 'penalty': 'l1', 'random_state': 0, 'solver': 'liblinear'}\n",
      "KNN params  {'algorithm': 'auto', 'leaf_size': 30, 'n_neighbors': 5, 'weights': 'distance'}\n",
      "DT params   {'criterion': 'gini', 'max_depth': 30, 'max_features': 'auto', 'random_state': 0, 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "print('LR params  ', LR_best.best_params_)\n",
    "print('KNN params ', KNN_best.best_params_)\n",
    "print('DT params  ', DT_best.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over Sampling \n",
      "\n",
      "LR Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0   0.738162  0.691906  0.714286       766\n",
      "           1   0.711491  0.755844  0.732997       770\n",
      "\n",
      "    accuracy                       0.723958      1536\n",
      "   macro avg   0.724827  0.723875  0.723642      1536\n",
      "weighted avg   0.724792  0.723958  0.723666      1536\n",
      "\n",
      "KNN Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0   0.987179  0.804178  0.886331       766\n",
      "           1   0.835526  0.989610  0.906064       770\n",
      "\n",
      "    accuracy                       0.897135      1536\n",
      "   macro avg   0.911353  0.896894  0.896198      1536\n",
      "weighted avg   0.911155  0.897135  0.896223      1536\n",
      "\n",
      "DT Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0   0.977044  0.889034  0.930964       766\n",
      "           1   0.898689  0.979221  0.937228       770\n",
      "\n",
      "    accuracy                       0.934245      1536\n",
      "   macro avg   0.937867  0.934127  0.934096      1536\n",
      "weighted avg   0.937765  0.934245  0.934104      1536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Over Sampling \\n')\n",
    "print(\"LR Classification Report: \\n\", class_rep(y_test_over, LR_preds_over, digits = 6))\n",
    "print(\"KNN Classification Report: \\n\", class_rep(y_test_over, KNN_preds_over, digits = 6))\n",
    "print(\"DT Classification Report: \\n\", class_rep(y_test_over, DT_preds_over, digits = 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшей моделью остается DecisionTree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT Confusion Matrix, Over Sampling\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# confusion matrix of DT model\n",
    "conf_matrix =  conf_matr(y_test_over, DT_preds_over)\n",
    "print('DT Confusion Matrix, Over Sampling')\n",
    "\n",
    "# visualisation\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = 'Blues', ax = ax, annot_kws = {'size': 20})\n",
    "ax.set_ylabel('FP       True label        TP')\n",
    "ax.set_xlabel('FN       Predicted label        TN')\n",
    "ax.xaxis.set_ticklabels(['1', '0'], fontsize = 10)\n",
    "ax.yaxis.set_ticklabels(['1', '0'], fontsize = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### ROC-AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9341273608897631"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_DT = roc_auc_score(y_test_over, DT_preds_over)\n",
    "roc_auc_DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission of .csv file with predictions\n",
    "submission_data = pd.DataFrame()\n",
    "submission_data['ID'] = X_test_over.index\n",
    "submission_data['Quality'] = DT_preds_over\n",
    "submission_data.to_csv('WineQualityPredictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Predict which wines are 'Good/1' and 'Not Good/0' (use binary classification; check balance of classes; calculate perdictions; choose the best model).\n",
    "\n",
    "**Answers**:\n",
    "\n",
    "1. Binary classification was applied.\n",
    "\n",
    "2. Classes were highly imbalanced with 78.36 % of '0' class and only 21.64 % of '1' class in our dataset. \n",
    "\n",
    "3. Three options were applied in order to calculate the best predictions:\n",
    "    * Calculate predictions with imbalanced dataset\n",
    "    * Calculate predictions with random undersampling technique of an imbalanced dataset\n",
    "    * Calculate predictions with random oversampling technique of an imbalanced dataset\n",
    "    \n",
    "4. Three ML models were used: Logistic Regression, KNN, Decision Tree (without and with hyper parameters).\n",
    "\n",
    "5. The best result was choosen: \n",
    "    * Random over-sampling dataset with 3838 enteties in class '0' and 3838 enteties in class '1', 7676 enteties in total.\n",
    "    * Train/Test split: test_size=0.2, random_state=0\n",
    "    * Decision Tree model:\n",
    "    1. with hyper parameters tuning - 'criterion': 'gini', 'max_depth': 30, 'max_features': 'auto', 'random_state': 0, 'splitter': 'best'\n",
    "    2. with an accuracy score equal 0.934245\n",
    "    3. ROC-AUC score equal 0.934127."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
